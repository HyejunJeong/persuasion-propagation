{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Persuasion Persistence Study - Unified Visualization\n",
    "\n",
    "## Overview\n",
    "This notebook provides comprehensive visualization and analysis for persuasion persistence experiments.\n",
    "\n",
    "## Sections\n",
    "1. **Configuration & Data Loading** - Setup and data import\n",
    "2. **Metric Calculation** - Friction scores, percentile ranks, behavioral metrics\n",
    "3. **Statistical Tests** - Mann-Whitney U, effect sizes, persona deltas\n",
    "4. **Core Visualizations** - Friction plots, behavioral comparisons, heatmaps\n",
    "5. **Experiment-Specific Analysis**\n",
    "   - Coding tasks (opinion_persuasion & prefill_only)\n",
    "   - Web research tasks (misaligned & aligned)\n",
    "\n",
    "## Supported Experiment Types\n",
    "- **Coding**: Opinion-persuasion vs Prefill-only\n",
    "- **Web (Misaligned)**: Unrelated opinion persuasion\n",
    "- **Web (Aligned)**: Task-aligned behavior persuasion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy.stats import mannwhitneyu, ttest_ind\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Baseline tactic name\n",
    "BASELINE = \"baseline\"\n",
    "\n",
    "# Small epsilon for numerical stability\n",
    "EPS = 1e-6\n",
    "\n",
    "# Color schemes\n",
    "TACTIC_COLORS = {\n",
    "    \"baseline\": \"#666666\",\n",
    "    \"logical_appeal\": \"#1f77b4\",\n",
    "    \"authority_endorsement\": \"#ff7f0e\",\n",
    "    \"evidence_based\": \"#2ca02c\",\n",
    "    \"priming_urgency\": \"#d62728\",\n",
    "    \"anchoring\": \"#9467bd\",\n",
    "    \"neutral_injection\": \"#8c564b\",\n",
    "}\n",
    "\n",
    "PERSONA_COLORS = {\n",
    "    \"gpt\": \"#1f77b4\",\n",
    "    \"claude\": \"#ff7f0e\",\n",
    "    \"llama\": \"#2ca02c\",\n",
    "    \"mistral\": \"#d62728\",\n",
    "    \"qwen\": \"#9467bd\",\n",
    "    \"gemini\": \"#8c564b\",\n",
    "    \"neutral\": \"#666666\",\n",
    "}\n",
    "\n",
    "# Default metrics for coding tasks\n",
    "CODING_RAW_METRICS = [\n",
    "    \"num_errors\",\n",
    "    \"num_code_revisions\",\n",
    "    \"coding_duration_s\",\n",
    "    \"revision_entropy\",\n",
    "    \"strategy_switch_rate\",\n",
    "    \"overcommitment\",\n",
    "    \"mean_revision_size\",\n",
    "    \"final_revision_delta\",\n",
    "]\n",
    "\n",
    "# Default metrics for web tasks\n",
    "WEB_RAW_METRICS = [\n",
    "    \"num_urls\",\n",
    "    \"num_unique_urls\",\n",
    "    \"num_domains\",\n",
    "    \"domain_entropy\",\n",
    "    \"num_searches\",\n",
    "    \"num_summaries\",\n",
    "    \"avg_latency_s\",\n",
    "    \"total_duration_s\",\n",
    "]\n",
    "\n",
    "print(\"✅ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA LOADING UTILITIES\n",
    "# ============================================================\n",
    "\n",
    "def load_jsonl(path: str, backbone: str = \"gpt\") -> pd.DataFrame:\n",
    "    \"\"\"Load JSONL file into dataframe with backbone label.\"\"\"\n",
    "    df = pd.read_json(path, lines=True)\n",
    "    df[\"backbone\"] = backbone\n",
    "    return df\n",
    "\n",
    "def load_multiple_files(file_dict: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"Load and concatenate multiple JSONL files.\n",
    "    \n",
    "    Args:\n",
    "        file_dict: Dict mapping persona/backbone name to file path\n",
    "    \n",
    "    Returns:\n",
    "        Combined dataframe\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for name, path in file_dict.items():\n",
    "        if Path(path).exists():\n",
    "            df = load_jsonl(path, backbone=name)\n",
    "            dfs.append(df)\n",
    "            print(f\"  ✓ Loaded {name}: {len(df)} rows\")\n",
    "        else:\n",
    "            print(f\"  ✗ File not found: {path}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        raise ValueError(\"No files were successfully loaded\")\n",
    "    \n",
    "    df_combined = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\n✅ Combined: {len(df_combined)} total rows\")\n",
    "    return df_combined\n",
    "\n",
    "def filter_baseline(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove baseline rows from dataframe.\"\"\"\n",
    "    return df[df[\"tactic\"] != BASELINE].copy()\n",
    "\n",
    "def validate_required_columns(df: pd.DataFrame, required: List[str]):\n",
    "    \"\"\"Check if dataframe has required columns.\"\"\"\n",
    "    missing = [col for col in required if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    print(f\"✅ All required columns present: {required}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics",
   "metadata": {},
   "source": [
    "## 3. Metric Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FRICTION SCORE CALCULATION\n",
    "# ============================================================\n",
    "\n",
    "def percentile_rank_nan_safe(x):\n",
    "    \"\"\"Calculate percentile ranks with NaN handling.\"\"\"\n",
    "    x = pd.Series(x)\n",
    "    out = pd.Series(np.nan, index=x.index, dtype=float)\n",
    "    mask = x.notna()\n",
    "    out.loc[mask] = x.loc[mask].rank(pct=True)\n",
    "    return out.values\n",
    "\n",
    "def compute_friction_score(\n",
    "    df: pd.DataFrame,\n",
    "    raw_metrics: List[str],\n",
    "    baseline_label: str = \"baseline\",\n",
    "    eps: float = EPS,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute friction score as mean percentile rank across metrics.\n",
    "    \n",
    "    Steps:\n",
    "    1. For each metric, compute percentile rank within each persona\n",
    "    2. Average percentile ranks across all metrics\n",
    "    3. Result is friction_score in [0, 1]\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        raw_metrics: List of raw metric column names\n",
    "        baseline_label: Name of baseline condition\n",
    "        eps: Small epsilon for numerical stability\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe with added friction_score column\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Compute percentile ranks for each metric\n",
    "    for metric in raw_metrics:\n",
    "        if metric not in df.columns:\n",
    "            print(f\"⚠️  Warning: {metric} not found in dataframe\")\n",
    "            continue\n",
    "        \n",
    "        rank_col = f\"{metric}_rank\"\n",
    "        df[rank_col] = df.groupby(\"persona\")[metric].transform(percentile_rank_nan_safe)\n",
    "    \n",
    "    # Compute mean percentile rank\n",
    "    rank_cols = [f\"{m}_rank\" for m in raw_metrics if f\"{m}_rank\" in df.columns]\n",
    "    df[\"friction_score\"] = df[rank_cols].mean(axis=1, skipna=True)\n",
    "    \n",
    "    print(f\"✅ Friction score computed (n={len(df)})\")\n",
    "    print(f\"   Mean: {df['friction_score'].mean():.3f}\")\n",
    "    print(f\"   Std: {df['friction_score'].std():.3f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def compute_normalized_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    raw_metrics: List[str],\n",
    "    baseline_label: str = \"baseline\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute baseline-normalized metrics: (x - baseline_mean) / baseline_std.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        raw_metrics: List of raw metric column names\n",
    "        baseline_label: Name of baseline condition\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe with added {metric}_norm columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    baseline_df = df[df[\"tactic\"] == baseline_label]\n",
    "    \n",
    "    for metric in raw_metrics:\n",
    "        if metric not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Compute baseline stats per persona\n",
    "        baseline_stats = baseline_df.groupby(\"persona\")[metric].agg([\"mean\", \"std\"])\n",
    "        \n",
    "        norm_col = f\"{metric}_norm\"\n",
    "        \n",
    "        def normalize_row(row):\n",
    "            persona = row[\"persona\"]\n",
    "            if persona not in baseline_stats.index:\n",
    "                return np.nan\n",
    "            mean_val = baseline_stats.loc[persona, \"mean\"]\n",
    "            std_val = baseline_stats.loc[persona, \"std\"]\n",
    "            if std_val == 0 or pd.isna(std_val):\n",
    "                return np.nan\n",
    "            return (row[metric] - mean_val) / std_val\n",
    "        \n",
    "        df[norm_col] = df.apply(normalize_row, axis=1)\n",
    "    \n",
    "    print(f\"✅ Normalized metrics computed\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistics",
   "metadata": {},
   "source": [
    "## 4. Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STATISTICAL TESTS\n",
    "# ============================================================\n",
    "\n",
    "def pooled_np_p_test(df: pd.DataFrame, score_col: str = \"friction_score\") -> Dict:\n",
    "    \"\"\"Compare not-persuaded vs persuaded groups using Mann-Whitney U test.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe with 'persuaded' column and score column\n",
    "        score_col: Name of score column to compare\n",
    "    \n",
    "    Returns:\n",
    "        Dict with test results\n",
    "    \"\"\"\n",
    "    np_vals = df[df[\"persuaded\"] == 0][score_col].dropna()\n",
    "    p_vals = df[df[\"persuaded\"] == 1][score_col].dropna()\n",
    "    \n",
    "    if len(np_vals) == 0 or len(p_vals) == 0:\n",
    "        return {\n",
    "            \"n_np\": len(np_vals),\n",
    "            \"n_p\": len(p_vals),\n",
    "            \"mean_np\": np.nan,\n",
    "            \"mean_p\": np.nan,\n",
    "            \"delta\": np.nan,\n",
    "            \"u_stat\": np.nan,\n",
    "            \"p_value\": np.nan,\n",
    "        }\n",
    "    \n",
    "    u_stat, p_value = mannwhitneyu(np_vals, p_vals, alternative=\"two-sided\")\n",
    "    \n",
    "    return {\n",
    "        \"n_np\": len(np_vals),\n",
    "        \"n_p\": len(p_vals),\n",
    "        \"mean_np\": np_vals.mean(),\n",
    "        \"mean_p\": p_vals.mean(),\n",
    "        \"delta\": p_vals.mean() - np_vals.mean(),\n",
    "        \"u_stat\": u_stat,\n",
    "        \"p_value\": p_value,\n",
    "    }\n",
    "\n",
    "def persona_delta_summary(df: pd.DataFrame, score_col: str = \"friction_score\") -> pd.DataFrame:\n",
    "    \"\"\"Compute per-persona differences between persuaded and not-persuaded.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe with 'persona', 'persuaded' columns\n",
    "        score_col: Score column to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Summary dataframe with per-persona statistics\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for persona, g in df.groupby(\"persona\"):\n",
    "        np_vals = g[g[\"persuaded\"] == 0][score_col].dropna()\n",
    "        p_vals = g[g[\"persuaded\"] == 1][score_col].dropna()\n",
    "        \n",
    "        if len(np_vals) == 0 or len(p_vals) == 0:\n",
    "            continue\n",
    "        \n",
    "        u_stat, p_value = mannwhitneyu(np_vals, p_vals, alternative=\"two-sided\")\n",
    "        \n",
    "        rows.append({\n",
    "            \"persona\": persona,\n",
    "            \"n_np\": len(np_vals),\n",
    "            \"n_p\": len(p_vals),\n",
    "            \"mean_np\": np_vals.mean(),\n",
    "            \"mean_p\": p_vals.mean(),\n",
    "            \"delta\": p_vals.mean() - np_vals.mean(),\n",
    "            \"u_stat\": u_stat,\n",
    "            \"p_value\": p_value,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def tactic_summary(df: pd.DataFrame, score_col: str = \"friction_score\") -> pd.DataFrame:\n",
    "    \"\"\"Summarize persuasion outcomes by tactic.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe with 'tactic', 'persuaded' columns\n",
    "        score_col: Score column to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Summary dataframe with per-tactic statistics\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for tactic, g in df.groupby(\"tactic\"):\n",
    "        if tactic == BASELINE:\n",
    "            continue\n",
    "        \n",
    "        n_total = len(g)\n",
    "        n_persuaded = g[\"persuaded\"].sum()\n",
    "        persuasion_rate = n_persuaded / n_total if n_total > 0 else 0\n",
    "        \n",
    "        np_vals = g[g[\"persuaded\"] == 0][score_col].dropna()\n",
    "        p_vals = g[g[\"persuaded\"] == 1][score_col].dropna()\n",
    "        \n",
    "        rows.append({\n",
    "            \"tactic\": tactic,\n",
    "            \"n_total\": n_total,\n",
    "            \"n_persuaded\": n_persuaded,\n",
    "            \"persuasion_rate\": persuasion_rate,\n",
    "            \"mean_np\": np_vals.mean() if len(np_vals) > 0 else np.nan,\n",
    "            \"mean_p\": p_vals.mean() if len(p_vals) > 0 else np.nan,\n",
    "            \"delta\": (p_vals.mean() - np_vals.mean()) if len(p_vals) > 0 and len(np_vals) > 0 else np.nan,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plotting",
   "metadata": {},
   "source": [
    "## 5. Core Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_friction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FRICTION SCORE PLOTS\n",
    "# ============================================================\n",
    "\n",
    "def plot_friction_by_persuasion(\n",
    "    df: pd.DataFrame,\n",
    "    score_col: str = \"friction_score\",\n",
    "    title: str = \"Friction Score: Not Persuaded vs Persuaded\",\n",
    "):\n",
    "    \"\"\"Box plot comparing friction scores by persuasion status.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    data_to_plot = [\n",
    "        df[df[\"persuaded\"] == 0][score_col].dropna(),\n",
    "        df[df[\"persuaded\"] == 1][score_col].dropna(),\n",
    "    ]\n",
    "    \n",
    "    bp = ax.boxplot(data_to_plot, labels=[\"Not Persuaded\", \"Persuaded\"], patch_artist=True)\n",
    "    \n",
    "    for patch, color in zip(bp['boxes'], ['#1f77b4', '#ff7f0e']):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_ylabel(\"Friction Score\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add statistical test result\n",
    "    test_result = pooled_np_p_test(df, score_col)\n",
    "    ax.text(0.02, 0.98, \n",
    "            f\"p = {test_result['p_value']:.4f}\\nΔ = {test_result['delta']:.3f}\",\n",
    "            transform=ax.transAxes, va='top', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_friction_by_tactic(\n",
    "    df: pd.DataFrame,\n",
    "    score_col: str = \"friction_score\",\n",
    "    title: str = \"Friction Score by Tactic\",\n",
    "):\n",
    "    \"\"\"Bar plot of mean friction scores by tactic and persuasion status.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    tactics = sorted([t for t in df[\"tactic\"].unique() if t != BASELINE])\n",
    "    \n",
    "    x = np.arange(len(tactics))\n",
    "    width = 0.35\n",
    "    \n",
    "    means_np = []\n",
    "    means_p = []\n",
    "    \n",
    "    for tactic in tactics:\n",
    "        g = df[df[\"tactic\"] == tactic]\n",
    "        means_np.append(g[g[\"persuaded\"] == 0][score_col].mean())\n",
    "        means_p.append(g[g[\"persuaded\"] == 1][score_col].mean())\n",
    "    \n",
    "    ax.bar(x - width/2, means_np, width, label='Not Persuaded', color='#1f77b4', alpha=0.7)\n",
    "    ax.bar(x + width/2, means_p, width, label='Persuaded', color='#ff7f0e', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Tactic')\n",
    "    ax.set_ylabel('Mean Friction Score')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(tactics, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_friction_by_persona(\n",
    "    df: pd.DataFrame,\n",
    "    score_col: str = \"friction_score\",\n",
    "    title: str = \"Friction Score by Persona\",\n",
    "):\n",
    "    \"\"\"Bar plot of mean friction scores by persona and persuasion status.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    personas = sorted(df[\"persona\"].unique())\n",
    "    \n",
    "    x = np.arange(len(personas))\n",
    "    width = 0.35\n",
    "    \n",
    "    means_np = []\n",
    "    means_p = []\n",
    "    \n",
    "    for persona in personas:\n",
    "        g = df[df[\"persona\"] == persona]\n",
    "        means_np.append(g[g[\"persuaded\"] == 0][score_col].mean())\n",
    "        means_p.append(g[g[\"persuaded\"] == 1][score_col].mean())\n",
    "    \n",
    "    ax.bar(x - width/2, means_np, width, label='Not Persuaded', alpha=0.7)\n",
    "    ax.bar(x + width/2, means_p, width, label='Persuaded', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Persona')\n",
    "    ax.set_ylabel('Mean Friction Score')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(personas)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_behavioral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BEHAVIORAL METRICS PLOTS\n",
    "# ============================================================\n",
    "\n",
    "def plot_behavioral_heatmap(\n",
    "    df: pd.DataFrame,\n",
    "    metrics: List[str],\n",
    "    title: str = \"Behavioral Metrics Heatmap\",\n",
    "):\n",
    "    \"\"\"Heatmap showing normalized behavioral metrics by tactic and persuasion status.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Compute mean for each metric by tactic and persuasion\n",
    "    heatmap_data = []\n",
    "    row_labels = []\n",
    "    \n",
    "    tactics = sorted([t for t in df[\"tactic\"].unique() if t != BASELINE])\n",
    "    \n",
    "    for tactic in tactics:\n",
    "        g = df[df[\"tactic\"] == tactic]\n",
    "        \n",
    "        np_row = []\n",
    "        p_row = []\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric not in df.columns:\n",
    "                np_row.append(np.nan)\n",
    "                p_row.append(np.nan)\n",
    "                continue\n",
    "            \n",
    "            np_vals = g[g[\"persuaded\"] == 0][metric].dropna()\n",
    "            p_vals = g[g[\"persuaded\"] == 1][metric].dropna()\n",
    "            \n",
    "            # Normalize by overall mean and std\n",
    "            overall_mean = df[metric].mean()\n",
    "            overall_std = df[metric].std()\n",
    "            \n",
    "            if overall_std > 0:\n",
    "                np_row.append((np_vals.mean() - overall_mean) / overall_std if len(np_vals) > 0 else 0)\n",
    "                p_row.append((p_vals.mean() - overall_mean) / overall_std if len(p_vals) > 0 else 0)\n",
    "            else:\n",
    "                np_row.append(0)\n",
    "                p_row.append(0)\n",
    "        \n",
    "        heatmap_data.append(np_row)\n",
    "        heatmap_data.append(p_row)\n",
    "        row_labels.append(f\"{tactic} (NP)\")\n",
    "        row_labels.append(f\"{tactic} (P)\")\n",
    "    \n",
    "    heatmap_array = np.array(heatmap_data)\n",
    "    \n",
    "    im = ax.imshow(heatmap_array, cmap='RdYlGn_r', aspect='auto', vmin=-2, vmax=2)\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(metrics)))\n",
    "    ax.set_yticks(np.arange(len(row_labels)))\n",
    "    ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(row_labels)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label='Normalized Value (σ)')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_metric_comparison(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    title: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Box plot comparing a single metric by persuasion status.\"\"\"\n",
    "    if title is None:\n",
    "        title = f\"{metric}: Not Persuaded vs Persuaded\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    data_to_plot = [\n",
    "        df[df[\"persuaded\"] == 0][metric].dropna(),\n",
    "        df[df[\"persuaded\"] == 1][metric].dropna(),\n",
    "    ]\n",
    "    \n",
    "    bp = ax.boxplot(data_to_plot, labels=[\"Not Persuaded\", \"Persuaded\"], patch_artist=True)\n",
    "    \n",
    "    for patch, color in zip(bp['boxes'], ['#1f77b4', '#ff7f0e']):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_prefill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREFILL-SPECIFIC PLOTS\n",
    "# ============================================================\n",
    "\n",
    "def plot_prefill_comparison(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    title: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Compare metric across prefill conditions (C0, NP, P).\"\"\"\n",
    "    if \"prefill_condition\" not in df.columns:\n",
    "        print(\"⚠️  No prefill_condition column found\")\n",
    "        return None\n",
    "    \n",
    "    if title is None:\n",
    "        title = f\"{metric} by Prefill Condition\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    conditions = [\"C0\", \"NP\", \"P\"]\n",
    "    data_to_plot = []\n",
    "    labels = []\n",
    "    \n",
    "    for cond in conditions:\n",
    "        vals = df[df[\"prefill_condition\"] == cond][metric].dropna()\n",
    "        if len(vals) > 0:\n",
    "            data_to_plot.append(vals)\n",
    "            labels.append(cond)\n",
    "    \n",
    "    if not data_to_plot:\n",
    "        print(\"⚠️  No data found for prefill conditions\")\n",
    "        return None\n",
    "    \n",
    "    bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "    \n",
    "    colors = ['#666666', '#1f77b4', '#ff7f0e']\n",
    "    for patch, color in zip(bp['boxes'], colors[:len(data_to_plot)]):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_xlabel('Prefill Condition')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example_usage",
   "metadata": {},
   "source": [
    "## 6. Example Usage\n",
    "\n",
    "Below are example analysis workflows for different experiment types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example_coding",
   "metadata": {},
   "source": [
    "### 6.1 Coding Tasks (Opinion-Persuasion Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example_coding_opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load and analyze coding task data (opinion-persuasion mode)\n",
    "\n",
    "# Load data\n",
    "coding_files = {\n",
    "    \"gpt\": \"coding-gpt-all.jsonl\",\n",
    "    \"claude\": \"coding-claude-all.jsonl\",\n",
    "    \"llama\": \"coding-llama-all.jsonl\",\n",
    "}\n",
    "\n",
    "# df_coding = load_multiple_files(coding_files)\n",
    "\n",
    "# Validate required columns\n",
    "# required_cols = [\"persona\", \"tactic\", \"persuaded\", \"persisted\"] + CODING_RAW_METRICS\n",
    "# validate_required_columns(df_coding, required_cols)\n",
    "\n",
    "# Compute friction score\n",
    "# df_coding = compute_friction_score(df_coding, CODING_RAW_METRICS)\n",
    "\n",
    "# Filter to non-baseline\n",
    "# df_coding_nobase = filter_baseline(df_coding)\n",
    "\n",
    "# Statistical tests\n",
    "# print(\"\\n=== Pooled Test ===\")\n",
    "# print(pooled_np_p_test(df_coding_nobase))\n",
    "\n",
    "# print(\"\\n=== Per-Persona Delta ===\")\n",
    "# print(persona_delta_summary(df_coding_nobase))\n",
    "\n",
    "# print(\"\\n=== Per-Tactic Summary ===\")\n",
    "# print(tactic_summary(df_coding_nobase))\n",
    "\n",
    "# Visualizations\n",
    "# plot_friction_by_persuasion(df_coding_nobase, title=\"Coding: Friction by Persuasion Status\")\n",
    "# plot_friction_by_tactic(df_coding_nobase, title=\"Coding: Friction by Tactic\")\n",
    "# plot_friction_by_persona(df_coding_nobase, title=\"Coding: Friction by Persona\")\n",
    "# plot_behavioral_heatmap(df_coding_nobase, CODING_RAW_METRICS, title=\"Coding: Behavioral Metrics Heatmap\")\n",
    "\n",
    "print(\"✅ Example coding analysis (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example_prefill",
   "metadata": {},
   "source": [
    "### 6.2 Coding Tasks (Prefill-Only Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example_coding_prefill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze prefill-only experiment\n",
    "\n",
    "# Load data\n",
    "# df_prefill = pd.read_json(\"prefill-coding-all.jsonl\", lines=True)\n",
    "\n",
    "# Compute metrics for each prefill condition\n",
    "# for metric in CODING_RAW_METRICS:\n",
    "#     plot_prefill_comparison(df_prefill, metric, title=f\"Coding Prefill: {metric}\")\n",
    "\n",
    "# Compare friction across conditions\n",
    "# df_prefill = compute_friction_score(df_prefill, CODING_RAW_METRICS)\n",
    "# plot_prefill_comparison(df_prefill, \"friction_score\", title=\"Coding Prefill: Friction Score\")\n",
    "\n",
    "print(\"✅ Example prefill analysis (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example_web",
   "metadata": {},
   "source": [
    "### 6.3 Web Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example_web_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load and analyze web research task data\n",
    "\n",
    "# Load data\n",
    "# df_web = pd.read_json(\"web-research-all.jsonl\", lines=True)\n",
    "\n",
    "# Compute friction score using web metrics\n",
    "# df_web = compute_friction_score(df_web, WEB_RAW_METRICS)\n",
    "\n",
    "# Filter to non-baseline\n",
    "# df_web_nobase = filter_baseline(df_web)\n",
    "\n",
    "# Visualizations\n",
    "# plot_friction_by_persuasion(df_web_nobase, title=\"Web: Friction by Persuasion Status\")\n",
    "# plot_behavioral_heatmap(df_web_nobase, WEB_RAW_METRICS, title=\"Web: Behavioral Metrics Heatmap\")\n",
    "\n",
    "# Individual metric comparisons\n",
    "# for metric in [\"num_unique_urls\", \"num_domains\", \"domain_entropy\"]:\n",
    "#     plot_metric_comparison(df_web_nobase, metric)\n",
    "\n",
    "print(\"✅ Example web analysis (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom",
   "metadata": {},
   "source": [
    "## 7. Custom Analysis\n",
    "\n",
    "Add your own analysis cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom analysis here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
