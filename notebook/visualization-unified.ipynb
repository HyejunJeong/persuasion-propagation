{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# Persuasion Persistence Study - Unified Visualization\n\n## Overview\nThis notebook provides comprehensive visualization and analysis for persuasion persistence experiments.\n\n## Sections\n1. **Configuration & Data Loading** - Setup and data import\n2. **Metric Calculation** - Percentile ranks, behavioral metrics\n3. **Statistical Tests** - Mann-Whitney U, effect sizes, persona deltas\n4. **Core Visualizations** - Behavioral comparisons, heatmaps\n5. **Experiment-Specific Analysis**\n   - Coding tasks (onthefly & prefill)\n   - Web research tasks (misaligned & aligned)\n\n## Supported Experiment Types\n- **Coding**: On-the-fly vs Prefill (uses TRS & EVS metrics)\n- **Web (Misaligned)**: Unrelated opinion persuasion\n- **Web (Aligned)**: Task-aligned behavior persuasion"
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy.stats import mannwhitneyu, ttest_ind\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Baseline tactic name\n",
    "BASELINE = \"baseline\"\n",
    "\n",
    "# Small epsilon for numerical stability\n",
    "EPS = 1e-6\n",
    "\n",
    "# Color schemes\n",
    "TACTIC_COLORS = {\n",
    "    \"baseline\": \"#666666\",\n",
    "    \"logical_appeal\": \"#1f77b4\",\n",
    "    \"authority_endorsement\": \"#ff7f0e\",\n",
    "    \"evidence_based\": \"#2ca02c\",\n",
    "    \"priming_urgency\": \"#d62728\",\n",
    "    \"anchoring\": \"#9467bd\",\n",
    "    \"neutral_injection\": \"#8c564b\",\n",
    "}\n",
    "\n",
    "PERSONA_COLORS = {\n",
    "    \"gpt\": \"#1f77b4\",\n",
    "    \"claude\": \"#ff7f0e\",\n",
    "    \"llama\": \"#2ca02c\",\n",
    "    \"mistral\": \"#d62728\",\n",
    "    \"qwen\": \"#9467bd\",\n",
    "    \"gemini\": \"#8c564b\",\n",
    "    \"neutral\": \"#666666\",\n",
    "}\n",
    "\n",
    "# Default metrics for coding tasks\n",
    "CODING_RAW_METRICS = [\n",
    "    \"num_errors\",\n",
    "    \"num_code_revisions\",\n",
    "    \"coding_duration_s\",\n",
    "    \"revision_entropy\",\n",
    "    \"strategy_switch_rate\",\n",
    "    \"overcommitment\",\n",
    "    \"mean_revision_size\",\n",
    "    \"final_revision_delta\",\n",
    "]\n",
    "\n",
    "# Default metrics for web tasks\n",
    "WEB_RAW_METRICS = [\n",
    "    \"num_urls\",\n",
    "    \"num_unique_urls\",\n",
    "    \"num_domains\",\n",
    "    \"domain_entropy\",\n",
    "    \"num_searches\",\n",
    "    \"num_summaries\",\n",
    "    \"avg_latency_s\",\n",
    "    \"total_duration_s\",\n",
    "]\n",
    "\n",
    "print(\"✅ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA LOADING UTILITIES\n",
    "# ============================================================\n",
    "\n",
    "def load_jsonl(path: str, backbone: str = \"gpt\") -> pd.DataFrame:\n",
    "    \"\"\"Load JSONL file into dataframe with backbone label.\"\"\"\n",
    "    df = pd.read_json(path, lines=True)\n",
    "    df[\"backbone\"] = backbone\n",
    "    return df\n",
    "\n",
    "def load_multiple_files(file_dict: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"Load and concatenate multiple JSONL files.\n",
    "    \n",
    "    Args:\n",
    "        file_dict: Dict mapping persona/backbone name to file path\n",
    "    \n",
    "    Returns:\n",
    "        Combined dataframe\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for name, path in file_dict.items():\n",
    "        if Path(path).exists():\n",
    "            df = load_jsonl(path, backbone=name)\n",
    "            dfs.append(df)\n",
    "            print(f\"  ✓ Loaded {name}: {len(df)} rows\")\n",
    "        else:\n",
    "            print(f\"  ✗ File not found: {path}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        raise ValueError(\"No files were successfully loaded\")\n",
    "    \n",
    "    df_combined = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\n✅ Combined: {len(df_combined)} total rows\")\n",
    "    return df_combined\n",
    "\n",
    "def filter_baseline(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove baseline rows from dataframe.\"\"\"\n",
    "    return df[df[\"tactic\"] != BASELINE].copy()\n",
    "\n",
    "def validate_required_columns(df: pd.DataFrame, required: List[str]):\n",
    "    \"\"\"Check if dataframe has required columns.\"\"\"\n",
    "    missing = [col for col in required if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    print(f\"✅ All required columns present: {required}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics",
   "metadata": {},
   "source": [
    "## 3. Metric Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friction",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# METRIC CALCULATION\n# ============================================================\n\ndef compute_normalized_metrics(\n    df: pd.DataFrame,\n    raw_metrics: List[str],\n    baseline_label: str = \"baseline\",\n) -> pd.DataFrame:\n    \"\"\"Compute baseline-normalized metrics: (x - baseline_mean) / baseline_std.\n    \n    Args:\n        df: Input dataframe\n        raw_metrics: List of raw metric column names\n        baseline_label: Name of baseline condition\n    \n    Returns:\n        Dataframe with added {metric}_norm columns\n    \"\"\"\n    df = df.copy()\n    \n    baseline_df = df[df[\"tactic\"] == baseline_label]\n    \n    for metric in raw_metrics:\n        if metric not in df.columns:\n            continue\n        \n        # Compute baseline stats per persona\n        baseline_stats = baseline_df.groupby(\"persona\")[metric].agg([\"mean\", \"std\"])\n        \n        norm_col = f\"{metric}_norm\"\n        \n        def normalize_row(row):\n            persona = row[\"persona\"]\n            if persona not in baseline_stats.index:\n                return np.nan\n            mean_val = baseline_stats.loc[persona, \"mean\"]\n            std_val = baseline_stats.loc[persona, \"std\"]\n            if std_val == 0 or pd.isna(std_val):\n                return np.nan\n            return (row[metric] - mean_val) / std_val\n        \n        df[norm_col] = df.apply(normalize_row, axis=1)\n    \n    print(f\"✅ Normalized metrics computed\")\n    return df"
  },
  {
   "cell_type": "markdown",
   "id": "statistics",
   "metadata": {},
   "source": [
    "## 4. Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# STATISTICAL TESTS\n# ============================================================\n\ndef pooled_np_p_test(df: pd.DataFrame, score_col: str) -> Dict:\n    \"\"\"Compare not-persuaded vs persuaded groups using Mann-Whitney U test.\n    \n    Args:\n        df: Dataframe with 'persuaded' column and score column\n        score_col: Name of score column to compare\n    \n    Returns:\n        Dict with test results\n    \"\"\"\n    np_vals = df[df[\"persuaded\"] == 0][score_col].dropna()\n    p_vals = df[df[\"persuaded\"] == 1][score_col].dropna()\n    \n    if len(np_vals) == 0 or len(p_vals) == 0:\n        return {\n            \"n_np\": len(np_vals),\n            \"n_p\": len(p_vals),\n            \"mean_np\": np.nan,\n            \"mean_p\": np.nan,\n            \"delta\": np.nan,\n            \"u_stat\": np.nan,\n            \"p_value\": np.nan,\n        }\n    \n    u_stat, p_value = mannwhitneyu(np_vals, p_vals, alternative=\"two-sided\")\n    \n    return {\n        \"n_np\": len(np_vals),\n        \"n_p\": len(p_vals),\n        \"mean_np\": np_vals.mean(),\n        \"mean_p\": p_vals.mean(),\n        \"delta\": p_vals.mean() - np_vals.mean(),\n        \"u_stat\": u_stat,\n        \"p_value\": p_value,\n    }\n\ndef persona_delta_summary(df: pd.DataFrame, score_col: str) -> pd.DataFrame:\n    \"\"\"Compute per-persona differences between persuaded and not-persuaded.\n    \n    Args:\n        df: Dataframe with 'persona', 'persuaded' columns\n        score_col: Score column to analyze\n    \n    Returns:\n        Summary dataframe with per-persona statistics\n    \"\"\"\n    rows = []\n    \n    for persona, g in df.groupby(\"persona\"):\n        np_vals = g[g[\"persuaded\"] == 0][score_col].dropna()\n        p_vals = g[g[\"persuaded\"] == 1][score_col].dropna()\n        \n        if len(np_vals) == 0 or len(p_vals) == 0:\n            continue\n        \n        u_stat, p_value = mannwhitneyu(np_vals, p_vals, alternative=\"two-sided\")\n        \n        rows.append({\n            \"persona\": persona,\n            \"n_np\": len(np_vals),\n            \"n_p\": len(p_vals),\n            \"mean_np\": np_vals.mean(),\n            \"mean_p\": p_vals.mean(),\n            \"delta\": p_vals.mean() - np_vals.mean(),\n            \"u_stat\": u_stat,\n            \"p_value\": p_value,\n        })\n    \n    return pd.DataFrame(rows)\n\ndef tactic_summary(df: pd.DataFrame, score_col: str) -> pd.DataFrame:\n    \"\"\"Summarize persuasion outcomes by tactic.\n    \n    Args:\n        df: Dataframe with 'tactic', 'persuaded' columns\n        score_col: Score column to analyze\n    \n    Returns:\n        Summary dataframe with per-tactic statistics\n    \"\"\"\n    rows = []\n    \n    for tactic, g in df.groupby(\"tactic\"):\n        if tactic == BASELINE:\n            continue\n        \n        n_total = len(g)\n        n_persuaded = g[\"persuaded\"].sum()\n        persuasion_rate = n_persuaded / n_total if n_total > 0 else 0\n        \n        np_vals = g[g[\"persuaded\"] == 0][score_col].dropna()\n        p_vals = g[g[\"persuaded\"] == 1][score_col].dropna()\n        \n        rows.append({\n            \"tactic\": tactic,\n            \"n_total\": n_total,\n            \"n_persuaded\": n_persuaded,\n            \"persuasion_rate\": persuasion_rate,\n            \"mean_np\": np_vals.mean() if len(np_vals) > 0 else np.nan,\n            \"mean_p\": p_vals.mean() if len(p_vals) > 0 else np.nan,\n            \"delta\": (p_vals.mean() - np_vals.mean()) if len(p_vals) > 0 and len(np_vals) > 0 else np.nan,\n        })\n    \n    return pd.DataFrame(rows)"
  },
  {
   "cell_type": "markdown",
   "id": "plotting",
   "metadata": {},
   "source": [
    "## 5. Core Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_behavioral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BEHAVIORAL METRICS PLOTS\n",
    "# ============================================================\n",
    "\n",
    "def plot_behavioral_heatmap(\n",
    "    df: pd.DataFrame,\n",
    "    metrics: List[str],\n",
    "    title: str = \"Behavioral Metrics Heatmap\",\n",
    "):\n",
    "    \"\"\"Heatmap showing normalized behavioral metrics by tactic and persuasion status.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Compute mean for each metric by tactic and persuasion\n",
    "    heatmap_data = []\n",
    "    row_labels = []\n",
    "    \n",
    "    tactics = sorted([t for t in df[\"tactic\"].unique() if t != BASELINE])\n",
    "    \n",
    "    for tactic in tactics:\n",
    "        g = df[df[\"tactic\"] == tactic]\n",
    "        \n",
    "        np_row = []\n",
    "        p_row = []\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric not in df.columns:\n",
    "                np_row.append(np.nan)\n",
    "                p_row.append(np.nan)\n",
    "                continue\n",
    "            \n",
    "            np_vals = g[g[\"persuaded\"] == 0][metric].dropna()\n",
    "            p_vals = g[g[\"persuaded\"] == 1][metric].dropna()\n",
    "            \n",
    "            # Normalize by overall mean and std\n",
    "            overall_mean = df[metric].mean()\n",
    "            overall_std = df[metric].std()\n",
    "            \n",
    "            if overall_std > 0:\n",
    "                np_row.append((np_vals.mean() - overall_mean) / overall_std if len(np_vals) > 0 else 0)\n",
    "                p_row.append((p_vals.mean() - overall_mean) / overall_std if len(p_vals) > 0 else 0)\n",
    "            else:\n",
    "                np_row.append(0)\n",
    "                p_row.append(0)\n",
    "        \n",
    "        heatmap_data.append(np_row)\n",
    "        heatmap_data.append(p_row)\n",
    "        row_labels.append(f\"{tactic} (NP)\")\n",
    "        row_labels.append(f\"{tactic} (P)\")\n",
    "    \n",
    "    heatmap_array = np.array(heatmap_data)\n",
    "    \n",
    "    im = ax.imshow(heatmap_array, cmap='RdYlGn_r', aspect='auto', vmin=-2, vmax=2)\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(metrics)))\n",
    "    ax.set_yticks(np.arange(len(row_labels)))\n",
    "    ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(row_labels)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label='Normalized Value (σ)')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_metric_comparison(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    title: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Box plot comparing a single metric by persuasion status.\"\"\"\n",
    "    if title is None:\n",
    "        title = f\"{metric}: Not Persuaded vs Persuaded\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    data_to_plot = [\n",
    "        df[df[\"persuaded\"] == 0][metric].dropna(),\n",
    "        df[df[\"persuaded\"] == 1][metric].dropna(),\n",
    "    ]\n",
    "    \n",
    "    bp = ax.boxplot(data_to_plot, labels=[\"Not Persuaded\", \"Persuaded\"], patch_artist=True)\n",
    "    \n",
    "    for patch, color in zip(bp['boxes'], ['#1f77b4', '#ff7f0e']):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_prefill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREFILL-SPECIFIC PLOTS\n",
    "# ============================================================\n",
    "\n",
    "def plot_prefill_comparison(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    title: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Compare metric across prefill conditions (C0, NP, P).\"\"\"\n",
    "    if \"prefill_condition\" not in df.columns:\n",
    "        print(\"⚠️  No prefill_condition column found\")\n",
    "        return None\n",
    "    \n",
    "    if title is None:\n",
    "        title = f\"{metric} by Prefill Condition\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    conditions = [\"C0\", \"NP\", \"P\"]\n",
    "    data_to_plot = []\n",
    "    labels = []\n",
    "    \n",
    "    for cond in conditions:\n",
    "        vals = df[df[\"prefill_condition\"] == cond][metric].dropna()\n",
    "        if len(vals) > 0:\n",
    "            data_to_plot.append(vals)\n",
    "            labels.append(cond)\n",
    "    \n",
    "    if not data_to_plot:\n",
    "        print(\"⚠️  No data found for prefill conditions\")\n",
    "        return None\n",
    "    \n",
    "    bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "    \n",
    "    colors = ['#666666', '#1f77b4', '#ff7f0e']\n",
    "    for patch, color in zip(bp['boxes'], colors[:len(data_to_plot)]):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_xlabel('Prefill Condition')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example_usage",
   "metadata": {},
   "source": [
    "## 6. Example Usage\n",
    "\n",
    "Below are example analysis workflows for different experiment types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example_coding",
   "metadata": {},
   "source": "### 6.1 Coding Tasks (On-the-Fly Mode)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example_coding_opinion",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Load and analyze coding task data (on-the-fly mode)\n\n# Load data\ncoding_files = {\n    \"gpt\": \"coding-gpt-all.jsonl\",\n    \"claude\": \"coding-claude-all.jsonl\",\n    \"llama\": \"coding-llama-all.jsonl\",\n}\n\n# df_coding = load_multiple_files(coding_files)\n\n# Validate required columns\n# required_cols = [\"persona\", \"tactic\", \"persuaded\", \"persisted\"] + CODING_RAW_METRICS\n# validate_required_columns(df_coding, required_cols)\n\n# Filter to non-baseline\n# df_coding_nobase = filter_baseline(df_coding)\n\n# Statistical tests for TRS (Task Revision Score)\n# print(\"\\n=== Pooled Test (TRS) ===\")\n# print(pooled_np_p_test(df_coding_nobase, score_col=\"trs\"))\n\n# print(\"\\n=== Per-Persona Delta (TRS) ===\")\n# print(persona_delta_summary(df_coding_nobase, score_col=\"trs\"))\n\n# Statistical tests for EVS (Exploration Variability Score)\n# print(\"\\n=== Pooled Test (EVS) ===\")\n# print(pooled_np_p_test(df_coding_nobase, score_col=\"evs\"))\n\n# print(\"\\n=== Per-Tactic Summary ===\")\n# print(tactic_summary(df_coding_nobase, score_col=\"trs\"))\n\n# Visualizations\n# plot_behavioral_heatmap(df_coding_nobase, CODING_RAW_METRICS, title=\"Coding: Behavioral Metrics Heatmap\")\n\nprint(\"✅ Example coding analysis (uncomment to run)\")"
  },
  {
   "cell_type": "markdown",
   "id": "example_prefill",
   "metadata": {},
   "source": "### 6.2 Coding Tasks (Prefill Mode)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example_coding_prefill",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Analyze prefill-only experiment\n\n# Load data\n# df_prefill = pd.read_json(\"prefill-coding-all.jsonl\", lines=True)\n\n# Compute metrics for each prefill condition\n# for metric in CODING_RAW_METRICS:\n#     plot_prefill_comparison(df_prefill, metric, title=f\"Coding Prefill: {metric}\")\n\n# Compare TRS and EVS across conditions\n# plot_prefill_comparison(df_prefill, \"trs\", title=\"Coding Prefill: TRS\")\n# plot_prefill_comparison(df_prefill, \"evs\", title=\"Coding Prefill: EVS\")\n\nprint(\"✅ Example prefill analysis (uncomment to run)\")"
  },
  {
   "cell_type": "markdown",
   "id": "example_web",
   "metadata": {},
   "source": [
    "### 6.3 Web Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example_web_analysis",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Load and analyze web research task data\n\n# Load data\n# df_web = pd.read_json(\"web-research-all.jsonl\", lines=True)\n\n# Filter to non-baseline\n# df_web_nobase = filter_baseline(df_web)\n\n# Visualizations\n# plot_behavioral_heatmap(df_web_nobase, WEB_RAW_METRICS, title=\"Web: Behavioral Metrics Heatmap\")\n\n# Individual metric comparisons\n# for metric in [\"num_unique_urls\", \"num_domains\", \"domain_entropy\"]:\n#     plot_metric_comparison(df_web_nobase, metric)\n\nprint(\"✅ Example web analysis (uncomment to run)\")"
  },
  {
   "cell_type": "markdown",
   "id": "custom",
   "metadata": {},
   "source": [
    "## 7. Custom Analysis\n",
    "\n",
    "Add your own analysis cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom analysis here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}